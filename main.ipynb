{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection With PyTorch and COCO Dataset\n",
    "\n",
    "### To-Do List:\n",
    "- The labels don't match when the image is flipped with the transforms. Fix it.\n",
    "- Add a training loop.\n",
    "- Add a testing loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as transformsV2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 5\n",
    "NUM_WORKERS = 0\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "classes = [\"__background__\", \"cup\"]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Paths\n",
    "path_to_dataset_root = Path(\"../datasets/cup_dataset\")\n",
    "path_to_coco = path_to_dataset_root / \"coco.json\"\n",
    "path_to_images = path_to_dataset_root / \"train\"\n",
    "path_to_test = path_to_dataset_root / \"test\"\n",
    "path_to_rest = path_to_dataset_root / \"cup\"\n",
    "\n",
    "# Transforms\n",
    "train_transform = transformsV2.Compose(transforms=[\n",
    "    transformsV2.Compose(transforms=[transformsV2.ToImage(), transformsV2.ToDtype(torch.float32, scale=True)]),\n",
    "    # # Commented out for now.\n",
    "    # transformsV2.RandomHorizontalFlip(p=1),\n",
    "    # transformsV2.RandomVerticalFlip(p=1),\n",
    "    transformsV2.RandomPhotometricDistort(p=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCODataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A Dataset class that utilises the CocoDetection class and grabs annotation data from a given coco file.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 path: Path,\n",
    "                 torch_coco_object: torchvision.datasets.CocoDetection,\n",
    "                 transforms: torchvision.transforms) -> None:\n",
    "        self.path = path\n",
    "        self.coco: torchvision.datasets.CocoDetection = torch_coco_object.coco\n",
    "        self.transforms: torchvision.transforms.v2 = transforms\n",
    "        self.all_images = list(path.glob(\"*.jpg\"))\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            (int): The length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.coco)\n",
    "    \n",
    "    def __getitem__(self, idx) -> tuple[torch.Tensor, dict]:\n",
    "        \"\"\"\n",
    "        Returns an image from the dataset and its annotations.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the image to be grabbed.\n",
    "            \n",
    "        Returns:\n",
    "            (tuple(torch.Tensor, dict)): A tuple containing the image and it's annotation data.\n",
    "        \"\"\"\n",
    "        # Get the image name\n",
    "        image_name: str = self.all_images[idx].name\n",
    "        \n",
    "        # Get the id from the image name\n",
    "        image_id: int = int(image_name.rstrip(\".jpg\"))\n",
    "        \n",
    "        # Get the batch using the image id\n",
    "        image = Image.open(self.path / image_name)\n",
    "        \n",
    "        # Initialise empty boxes and categories lists.\n",
    "        boxes, categories = [], []\n",
    "        \n",
    "        # For every annotation in the coco annotations\n",
    "        for index in self.coco.anns:\n",
    "            # If the image_id generated from the image name matches the image_id of the annotation\n",
    "            if self.coco.anns[index][\"image_id\"] == image_id:\n",
    "                # Add the bounding box coordinates to the boxes list.\n",
    "                boxes.append(self.coco.anns[index][\"bbox\"])\n",
    "                \n",
    "                # Add the category to the categories list. \n",
    "                categories.append(self.coco.anns[index][\"category_id\"])\n",
    "        \n",
    "        # Create the targets dictionary\n",
    "        targets = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": categories\n",
    "        }\n",
    "        \n",
    "        # If there are any transforms provided, apply them on both the targets and the image\n",
    "        if self.transforms is not None:\n",
    "            return self.transforms(image, targets)\n",
    "\n",
    "        # Else return the image and targets as is.\n",
    "        return image, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.42s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "def draw_from_coco(path_to_images: Path,\n",
    "                   path_to_coco: Path,\n",
    "                   transform: transformsV2,\n",
    "                   classes: list,\n",
    "                   idx: int) -> None:\n",
    "    \"\"\"\n",
    "    Draws to an image using its annotations grabbed from a COCO annotation file.\n",
    "    \n",
    "    Args:\n",
    "        path_to_images (Path): Path to the images that contains the annotated images.\n",
    "        path_to_coco (Path): Path to the annotation file.\n",
    "        transform (torchvision.transforms.v2): Transforms to be applied on the images.\n",
    "        classes (list): The list of classes.\n",
    "        idx (int): The index of the image to be drawn on.\n",
    "    \"\"\"\n",
    "    \n",
    "    torch_coco_object = torchvision.datasets.CocoDetection(root=path_to_images,\n",
    "                                                           annFile=path_to_coco,\n",
    "                                                           transforms=transform)\n",
    "\n",
    "    coco_dataset = COCODataset(path=path_to_images, torch_coco_object=torch_coco_object, transforms=transform)\n",
    "    batch = coco_dataset.__getitem__(idx)\n",
    "    tensor_image, anns = batch\n",
    "\n",
    "\n",
    "    boxes, labels = [], []\n",
    "\n",
    "    for box in anns[\"boxes\"]:\n",
    "        bbox = torchvision.ops.box_convert(boxes=torch.as_tensor(box),\n",
    "                                        in_fmt=\"xywh\",\n",
    "                                        out_fmt=\"xyxy\")\n",
    "        boxes.append(bbox)\n",
    "\n",
    "    labels = [classes[label] for label in anns[\"labels\"]]\n",
    "    boxes = torch.stack(boxes)\n",
    "    drawn_image = torchvision.utils.draw_bounding_boxes(image=tensor_image,\n",
    "                                                        boxes=boxes,\n",
    "                                                        labels=labels,\n",
    "                                                        colors=(255,0,0))\n",
    "\n",
    "    pil_image = torchvision.transforms.v2.ToPILImage()(drawn_image)\n",
    "    Image._show(pil_image)\n",
    "    \n",
    "draw_from_coco(path_to_images=path_to_images,\n",
    "               path_to_coco=path_to_coco,\n",
    "               transform=train_transform,\n",
    "               classes=classes,\n",
    "               idx=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
