{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection With PyTorch and COCO Dataset\n",
    "\n",
    "### To-Do List:\n",
    "- Fix collate_fn and dataset classes\n",
    "    - Dataset class needs to return a tuple of images and labels\n",
    "    - collate_fn's return value is wrong.\n",
    "- Check TVTensors\n",
    "- The labels don't match when the image is flipped with the transforms. Fix it.\n",
    "- Add a training loop.\n",
    "- Add a testing loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as transformsV2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 5\n",
    "NUM_WORKERS = 0\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "classes = [\"__background__\", \"cup\"]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = torchvision.models.detection.FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(weights=weights)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                             lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                            step_size=5,\n",
    "                                            gamma=0.1)\n",
    "\n",
    "# Paths\n",
    "path_to_dataset_root = Path(\"../datasets/cup_dataset\")\n",
    "path_to_coco = path_to_dataset_root / \"coco.json\"\n",
    "path_to_train = path_to_dataset_root / \"train\"\n",
    "path_to_validation = path_to_dataset_root / \"validation\"\n",
    "path_to_rest = path_to_dataset_root / \"cup\"\n",
    "\n",
    "# Transforms\n",
    "train_transform = transformsV2.Compose(transforms=[\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    transformsV2.RandomPhotometricDistort(p=1)\n",
    "])\n",
    "\n",
    "test_transform = transformsV2.Compose(transforms=[\n",
    "    transformsV2.Compose(transforms=[transformsV2.ToImage(), transformsV2.ToDtype(torch.float32, scale=True)]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable-sized inputs in the DataLoader.\n",
    "\n",
    "    Args:\n",
    "        batch (list): A list of tuples (image_tensor, labels).\n",
    "\n",
    "    Returns:\n",
    "        Tuple(torch.Tensor, List): A tuple containing a batch of images and a list of labels.\n",
    "    \"\"\"\n",
    "    # Separate the images and the labels\n",
    "    images, labels = zip(*batch)\n",
    "    \n",
    "    # Stack images into a single tensor\n",
    "    images = torch.stack(images)\n",
    "\n",
    "    # Initialize lists for boxes and classes\n",
    "    boxes = []\n",
    "    classes = []\n",
    "\n",
    "    # Iterate through each label and append to respective lists\n",
    "    for label in labels:\n",
    "        for box in label[\"boxes\"]:\n",
    "            boxes.append(box)\n",
    "        for _class in label[\"labels\"]:\n",
    "            classes.append(_class)\n",
    "            \n",
    "    print(boxes, classes)\n",
    "\n",
    "    return images, {'boxes': boxes, 'labels': classes}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # TODO: Get the biggest tuple's length and set it as the maximum length, and pad the rest of the batches with 0\n",
    "    # [(1,1), (1,1,1), (1,1,1,1)] -> [(1,1,0,0), (1,1,1,0), (1,1,1,1)]\n",
    "    for item in batch:\n",
    "        image = item[\"image\"]\n",
    "        labels = item[\"labels\"]\n",
    "        max_len = len(labels)\n",
    "    print(image, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.44s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data\n",
    "import torchvision.transforms.functional\n",
    "\n",
    "\n",
    "class COCODataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A Dataset class that utilises the CocoDetection class and grabs annotation data from a given coco file.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 path: Path,\n",
    "                 torch_coco_object: torchvision.datasets.CocoDetection,\n",
    "                 transforms: torchvision.transforms) -> None:\n",
    "        self.path = path\n",
    "        self.coco: torchvision.datasets.CocoDetection = torch_coco_object.coco\n",
    "        self.transforms: torchvision.transforms.v2 = transforms\n",
    "        self.all_images = list(path.glob(\"*.jpg\"))\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            (int): The length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.all_images)\n",
    "  \n",
    "    \n",
    "    def __getitem__(self, idx) -> tuple[torch.Tensor, dict]:\n",
    "        \"\"\"\n",
    "        Returns an image from the dataset and its annotations.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the image to be grabbed.\n",
    "            \n",
    "        Returns:\n",
    "            (tuple(torch.Tensor, dict)): A tuple containing the image and it's annotation data.\n",
    "        \"\"\"\n",
    "        # Get the image name\n",
    "        image_name: str = self.all_images[idx].name\n",
    "        \n",
    "        # Get the id from the image name\n",
    "        image_id: int = int(image_name.rstrip(\".jpg\"))\n",
    "        \n",
    "        # Get the batch using the image id\n",
    "        image = Image.open(self.path / image_name)\n",
    "        \n",
    "        # Initialise empty boxes and categories lists.\n",
    "        boxes, categories = [], []\n",
    "        \n",
    "        # For every annotation in the coco annotations\n",
    "        for index in self.coco.anns:\n",
    "            # If the image_id generated from the image name matches the image_id of the annotation\n",
    "            if self.coco.anns[index][\"image_id\"] == image_id:\n",
    "                # Add the bounding box coordinates to the boxes list.\n",
    "                boxes.append(self.coco.anns[index][\"bbox\"])\n",
    "                \n",
    "                # Add the category to the categories list. \n",
    "                categories.append(self.coco.anns[index][\"category_id\"])\n",
    "\n",
    "        targets = {\n",
    "            \"boxes\": boxes,\n",
    "            \"categories\": categories\n",
    "        }\n",
    "        \n",
    "        sample = {\n",
    "            \"image\": image,\n",
    "            \"labels\": targets\n",
    "        }\n",
    "        \n",
    "        # If there are any transforms provided, apply them on transforms and targets\n",
    "        if self.transforms is not None:\n",
    "            image, targets = self._apply_transforms(image=image, targets=sample[\"labels\"])\n",
    "            sample[\"image\"] = image\n",
    "            return sample\n",
    "\n",
    "        # Else return the image and targets as is.\n",
    "        return sample\n",
    "    \n",
    "        \n",
    "    def _apply_transforms(self, image, targets):\n",
    "        targets[\"boxes\"] = torch.tensor(targets[\"boxes\"], dtype=torch.float32)\n",
    "        # Apply horizontal flip\n",
    "        if torch.rand(1).item() < 0.5:\n",
    "            image = torchvision.transforms.functional.hflip(image)\n",
    "            width, _ = image.size\n",
    "            targets[\"boxes\"][:, 0] = width - targets[\"boxes\"][:, 0] - targets[\"boxes\"][:, 2]\n",
    "        \n",
    "        # Apply vertical flip\n",
    "        if torch.rand(1).item() < 0.5:\n",
    "            image = torchvision.transforms.functional.vflip(image)\n",
    "            _, height = image.size\n",
    "            targets[\"boxes\"][:, 1] = height - targets[\"boxes\"][:, 1] - targets[\"boxes\"][:, 3]\n",
    "        \n",
    "        # Other transformations can be applied similarly\n",
    "        image = self.transforms(image)\n",
    "        return image, targets\n",
    "\n",
    "\n",
    "torch_coco_object = torchvision.datasets.CocoDetection(root=path_to_train, annFile=path_to_coco, transform=train_transform)\n",
    "\n",
    "train_dataset = COCODataset(path=path_to_train,\n",
    "                            torch_coco_object=torch_coco_object,\n",
    "                            transforms=train_transform)\n",
    "validation_dataset = COCODataset(path=path_to_validation,\n",
    "                           torch_coco_object=torch_coco_object,\n",
    "                           transforms=test_transform)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True,\n",
    "                                               collate_fn=collate_fn)\n",
    "\n",
    "validation_dataloader = torch.utils.data.DataLoader(dataset=validation_dataset,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               shuffle=False,\n",
    "                                               collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_from_coco(coco_dataset: COCODataset,\n",
    "                   classes: list,\n",
    "                   idx: int) -> None:\n",
    "    \"\"\"\n",
    "    Draws to an image using its annotations grabbed from a COCO annotation file.\n",
    "    \n",
    "    Args:\n",
    "        path_to_images (Path): Path to the images that contains the annotated images.\n",
    "        path_to_coco (Path): Path to the annotation file.\n",
    "        transform (torchvision.transforms.v2): Transforms to be applied on the images.\n",
    "        classes (list): The list of classes.\n",
    "        idx (int): The index of the image to be drawn on.\n",
    "    \"\"\"\n",
    "\n",
    "    # Grab a batch from the dataset\n",
    "    batch = coco_dataset.__getitem__(idx)    \n",
    "    image = batch[\"image\"]\n",
    "    anns = batch[\"labels\"]\n",
    "    \n",
    "    # Change the format of the bounding box coordinates and append them into a box list\n",
    "    boxes, labels = [], []\n",
    "    for box in anns[\"boxes\"]:\n",
    "        bbox = torchvision.ops.box_convert(boxes=torch.as_tensor(box),\n",
    "                                        in_fmt=\"xywh\",\n",
    "                                        out_fmt=\"xyxy\")\n",
    "        boxes.append(bbox)\n",
    "\n",
    "    # Create a labels list from the aformentioned annotations\n",
    "    labels = [classes[label] for label in anns[\"labels\"]]\n",
    "    \n",
    "    # Stack the boxes \n",
    "    boxes = torch.stack(boxes)\n",
    "    \n",
    "    # Draw the labels onto the image\n",
    "    drawn_image = torchvision.utils.draw_bounding_boxes(image=image,\n",
    "                                                        boxes=boxes,\n",
    "                                                        labels=labels,\n",
    "                                                        colors=(255,0,0))\n",
    "    # Turn the drawn tensor image into a PIL image\n",
    "    pil_image = torchvision.transforms.v2.ToPILImage()(drawn_image)\n",
    "    \n",
    "    # Show the image\n",
    "    Image._show(pil_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3400, 0.3441, 0.3483,  ..., 0.3449, 0.3573, 0.3696],\n",
      "         [0.3400, 0.3400, 0.3400,  ..., 0.3407, 0.3490, 0.3531],\n",
      "         [0.3359, 0.3359, 0.3318,  ..., 0.3325, 0.3366, 0.3366],\n",
      "         ...,\n",
      "         [0.3755, 0.3796, 0.3837,  ..., 0.3959, 0.3916, 0.3872],\n",
      "         [0.3920, 0.3961, 0.4044,  ..., 0.3746, 0.3787, 0.3829],\n",
      "         [0.3713, 0.3796, 0.3879,  ..., 0.3457, 0.3540, 0.3663]],\n",
      "\n",
      "        [[0.0009, 0.0009, 0.0009,  ..., 0.0009, 0.0009, 0.0009],\n",
      "         [0.0009, 0.0009, 0.0009,  ..., 0.0009, 0.0009, 0.0009],\n",
      "         [0.0009, 0.0009, 0.0009,  ..., 0.0009, 0.0009, 0.0009],\n",
      "         ...,\n",
      "         [0.0009, 0.0009, 0.0009,  ..., 0.0142, 0.0098, 0.0055],\n",
      "         [0.0009, 0.0009, 0.0009,  ..., 0.0009, 0.0009, 0.0011],\n",
      "         [0.0009, 0.0009, 0.0009,  ..., 0.0009, 0.0009, 0.0009]],\n",
      "\n",
      "        [[0.4454, 0.4498, 0.4541,  ..., 0.4396, 0.4526, 0.4657],\n",
      "         [0.4454, 0.4454, 0.4454,  ..., 0.4352, 0.4439, 0.4483],\n",
      "         [0.4411, 0.4411, 0.4367,  ..., 0.4265, 0.4309, 0.4309],\n",
      "         ...,\n",
      "         [0.4718, 0.4761, 0.4805,  ..., 0.4872, 0.4828, 0.4785],\n",
      "         [0.4892, 0.4935, 0.5022,  ..., 0.4654, 0.4698, 0.4741],\n",
      "         [0.4674, 0.4761, 0.4848,  ..., 0.4350, 0.4437, 0.4567]]]) {'boxes': tensor([[129.5000, 126.5000, 129.0000, 108.0000]]), 'categories': [1]}\n"
     ]
    }
   ],
   "source": [
    "batch = [train_dataset.__getitem__(0), train_dataset.__getitem__(1)]\n",
    "collate_fn(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got dict",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 36\u001b[0m\n\u001b[0;32m     32\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\n\u001b[1;32m---> 36\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m           \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m           \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m           \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[73], line 17\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, data_loader, optimizer, device)\u001b[0m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set the model to training mode\u001b[39;00m\n\u001b[0;32m     15\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 17\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Move images and targets to the device\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\pytorch-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\pytorch-env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\pytorch-env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 34\u001b[0m, in \u001b[0;36mcollate_fn\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     32\u001b[0m tensors, targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m     33\u001b[0m features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpad_sequence(tensors, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 34\u001b[0m targets \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features, targets\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got dict"
     ]
    }
   ],
   "source": [
    "def train_step(model, data_loader, optimizer, device):\n",
    "    \"\"\"\n",
    "    Performs one training step for the object detection model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The Faster R-CNN model.\n",
    "        data_loader (torch.utils.data.DataLoader): The DataLoader providing the training data.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer for the model.\n",
    "        device (torch.device): The device (CPU or GPU) to run the training on.\n",
    "\n",
    "    Returns:\n",
    "        float: The total loss for the step.\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for images, targets in data_loader:\n",
    "        # Move images and targets to the device\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        \n",
    "        # Calculate total loss\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        total_loss += losses.item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "loss = train_step(model=model,\n",
    "           data_loader=train_dataloader,\n",
    "           optimizer=optimizer,\n",
    "           device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
